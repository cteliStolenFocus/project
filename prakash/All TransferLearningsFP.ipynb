{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da3874a",
   "metadata": {},
   "source": [
    "#### AAI 521 - <b><font color='Red'>Final Project</font></b>\n",
    "- This module author: <b><font color='Red'>PRAKASH PERIMBETI</font></b>\n",
    "- TEAM: Christopher Teli (Lead), Olympia Saha and Prakash Perimbeti\n",
    "- Date: December 12, 2022\n",
    "- Professor: Dr. Saeed Sardari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f2c9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 20:38:55.472470: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-08 20:38:55.472722: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-08 20:38:55.472756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, Markdown\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from time import perf_counter\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.layers as tfl\n",
    "from keras.optimizers import SGD   \n",
    "from  sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6de02bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4972 4972\n",
      "train/N/088.jpg N\n",
      "(4972, 2)\n",
      "(4972, 2) (4474, 2) (498, 2)\n",
      "Found 4027 validated image filenames belonging to 24 classes.\n",
      "Found 447 validated image filenames belonging to 24 classes.\n",
      "Found 498 validated image filenames belonging to 24 classes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get image file names and its classifiers\n",
    "trainimage_dir = Path('./train')\n",
    "trainlabels = []\n",
    "# Get filepaths and labels\n",
    "#filepaths =  os.listdir(image_dir)\n",
    "Tfilepaths = list(trainimage_dir.glob(r'**/*.jpg'))\n",
    "count = 0 \n",
    "\n",
    "for filename in filepaths:\n",
    "    lbl = str(filename)[6:7]\n",
    "    trainlabels.append(lbl)\n",
    "    count+=1\n",
    "\n",
    "print(len(Tfilepaths), len(trainlabels))\n",
    "print(Tfilepaths[4000], trainlabels[4000])\n",
    "\n",
    "\n",
    "# total 2515 images\n",
    "filepaths = pd.Series(Tfilepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(trainlabels, name='Label')\n",
    "\n",
    "# Concatenate filepaths and labels\n",
    "T_IMAGE_DF = pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "# Shuffle the DataFrame and reset index\n",
    "T_IMAGE_DF = T_IMAGE_DF.sample(frac=1).reset_index(drop = True)\n",
    "\n",
    "#le_Label_ACTIVE = LabelEncoder()\n",
    "#IMAGE_DF['Label'] = le_Label_ACTIVE.fit_transform(IMAGE_DF['Label'])\n",
    "\n",
    "#IMAGE_DF['Label'] = pd.Categorical(IMAGE_DF.Label)\n",
    "print(T_IMAGE_DF.shape)\n",
    "# Show the result\n",
    "T_IMAGE_DF.head(5)\n",
    "# Separate in train and test data\n",
    "T_TRAIN_DF, T_TEST_DF = train_test_split(T_IMAGE_DF, train_size=0.9, shuffle=True, random_state=1)\n",
    "print(T_IMAGE_DF.shape, T_TRAIN_DF.shape, T_TEST_DF.shape)\n",
    "# Create the generators\n",
    "T_train_generator,T_test_generator,T_TRAIN_IMAGES,T_VAL_IMAGES,T_TEST_IMAGES=create_gen(T_TRAIN_DF, T_TEST_DF)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c510f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get image file names and its classifiers\n",
    "image_dir = Path('./imagedata')\n",
    "labels = []\n",
    "# Get filepaths and labels\n",
    "#filepaths =  os.listdir(image_dir)\n",
    "filepaths = list(image_dir.glob(r'**/*.png'))\n",
    "count = 0 \n",
    "for filename in filepaths:\n",
    "    labels.append(str(filename[6]))\n",
    "    filepaths[count] = './imagedata' + '/' + filename\n",
    "    count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d75d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2515, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./imagedata/hand1_c_dif_seg_5_cropped.png</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./imagedata/hand3_b_dif_seg_2_cropped.png</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./imagedata/hand1_u_top_seg_4_cropped.png</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./imagedata/hand2_a_dif_seg_1_cropped.png</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./imagedata/hand2_f_left_seg_3_cropped.png</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Filepath Label\n",
       "0   ./imagedata/hand1_c_dif_seg_5_cropped.png     c\n",
       "1   ./imagedata/hand3_b_dif_seg_2_cropped.png     b\n",
       "2   ./imagedata/hand1_u_top_seg_4_cropped.png     u\n",
       "3   ./imagedata/hand2_a_dif_seg_1_cropped.png     a\n",
       "4  ./imagedata/hand2_f_left_seg_3_cropped.png     f"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn.preprocessing import LabelEncoder \n",
    "# total 2515 images\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "# Concatenate filepaths and labels\n",
    "IMAGE_DF = pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "# Shuffle the DataFrame and reset index\n",
    "IMAGE_DF = IMAGE_DF.sample(frac=1).reset_index(drop = True)\n",
    "\n",
    "#le_Label_ACTIVE = LabelEncoder()\n",
    "#IMAGE_DF['Label'] = le_Label_ACTIVE.fit_transform(IMAGE_DF['Label'])\n",
    "\n",
    "#IMAGE_DF['Label'] = pd.Categorical(IMAGE_DF.Label)\n",
    "print(IMAGE_DF.shape)\n",
    "# Show the result\n",
    "IMAGE_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "619c6e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2515 2515 r\n"
     ]
    }
   ],
   "source": [
    "print(len(filepaths), len(labels), labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f65865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2515, 2) (2515, 2) (2515, 2)\n"
     ]
    }
   ],
   "source": [
    "# Separate in train and test data\n",
    "TRAIN_DF, TEST_DF = train_test_split(IMAGE_DF, train_size=0.9, shuffle=True, random_state=1)\n",
    "print(IMAGE_DF.shape, IMAGE_DF.shape, IMAGE_DF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47de28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8586586550\n",
    "#\n",
    "# This loads images using Image Data Generator\n",
    "def create_gen(train_df, test_df):\n",
    "    # Load the Images with a generator and Data Augmentation\n",
    "    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        validation_split=0.1\n",
    "    )\n",
    "\n",
    "    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    )\n",
    "\n",
    "    train_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "        subset='training',\n",
    "        rotation_range=30, # Uncomment to use data augmentation\n",
    "        zoom_range=0.15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    val_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "        subset='validation',\n",
    "        rotation_range=30, # Uncomment to use data augmentation\n",
    "        zoom_range=0.15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    test_images = test_generator.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_generator,test_generator,train_images,val_images,test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3d5198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2037 validated image filenames belonging to 36 classes.\n",
      "Found 226 validated image filenames belonging to 36 classes.\n",
      "Found 252 validated image filenames belonging to 36 classes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the generators\n",
    "train_generator,test_generator,TRAIN_IMAGES,VAL_IMAGES,TEST_IMAGES=create_gen(TRAIN_DF, TEST_DF)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e5ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: InceptionResNetV2\n",
      "fitting the model: InceptionResNetV2\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 262s 4s/step - loss: 3.6287 - accuracy: 0.0196 - val_loss: 3.6198 - val_accuracy: 0.0310\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 218s 3s/step - loss: 3.5465 - accuracy: 0.0422 - val_loss: 3.5625 - val_accuracy: 0.0310\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 217s 3s/step - loss: 3.5042 - accuracy: 0.0555 - val_loss: 3.5255 - val_accuracy: 0.0487\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 218s 3s/step - loss: 3.4727 - accuracy: 0.0717 - val_loss: 3.4980 - val_accuracy: 0.0929\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 217s 3s/step - loss: 3.4448 - accuracy: 0.1011 - val_loss: 3.4719 - val_accuracy: 0.1283\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 218s 3s/step - loss: 3.4183 - accuracy: 0.1306 - val_loss: 3.4472 - val_accuracy: 0.1372\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 218s 3s/step - loss: 3.3909 - accuracy: 0.1453 - val_loss: 3.4231 - val_accuracy: 0.1460\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 218s 3s/step - loss: 3.3624 - accuracy: 0.1566 - val_loss: 3.3989 - val_accuracy: 0.1549\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 219s 3s/step - loss: 3.3310 - accuracy: 0.1586 - val_loss: 3.3697 - val_accuracy: 0.1726\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 219s 3s/step - loss: 3.3002 - accuracy: 0.1699 - val_loss: 3.3411 - val_accuracy: 0.1637\n",
      "InceptionResNetV2    trained in 2225.92 sec\n",
      "processing: ResNet101\n",
      "fitting the model: ResNet101\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 318s 5s/step - loss: 3.5979 - accuracy: 0.0285 - val_loss: 3.5882 - val_accuracy: 0.0177\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 293s 5s/step - loss: 3.5673 - accuracy: 0.0329 - val_loss: 3.5782 - val_accuracy: 0.0133\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 294s 5s/step - loss: 3.5554 - accuracy: 0.0501 - val_loss: 3.5701 - val_accuracy: 0.0133\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 293s 5s/step - loss: 3.5484 - accuracy: 0.0461 - val_loss: 3.5610 - val_accuracy: 0.0044\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 294s 5s/step - loss: 3.5435 - accuracy: 0.0466 - val_loss: 3.5575 - val_accuracy: 0.0133\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 295s 5s/step - loss: 3.5383 - accuracy: 0.0471 - val_loss: 3.5527 - val_accuracy: 0.0265\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 295s 5s/step - loss: 3.5335 - accuracy: 0.0574 - val_loss: 3.5467 - val_accuracy: 0.0310\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 292s 5s/step - loss: 3.5288 - accuracy: 0.0545 - val_loss: 3.5416 - val_accuracy: 0.0265\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 290s 5s/step - loss: 3.5245 - accuracy: 0.0584 - val_loss: 3.5394 - val_accuracy: 0.0310\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 291s 5s/step - loss: 3.5202 - accuracy: 0.0574 - val_loss: 3.5355 - val_accuracy: 0.0354\n",
      "ResNet101            trained in 2958.19 sec\n",
      "processing: InceptionV3\n",
      "fitting the model: InceptionV3\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 112s 2s/step - loss: 3.7084 - accuracy: 0.0118 - val_loss: 3.6171 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.5691 - accuracy: 0.0353 - val_loss: 3.5508 - val_accuracy: 0.0221\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.5136 - accuracy: 0.0535 - val_loss: 3.5096 - val_accuracy: 0.0398\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.4714 - accuracy: 0.0707 - val_loss: 3.4741 - val_accuracy: 0.0487\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.4326 - accuracy: 0.0918 - val_loss: 3.4372 - val_accuracy: 0.0708\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.3929 - accuracy: 0.1139 - val_loss: 3.3974 - val_accuracy: 0.0973\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.3521 - accuracy: 0.1247 - val_loss: 3.3600 - val_accuracy: 0.1195\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.3104 - accuracy: 0.1473 - val_loss: 3.3230 - val_accuracy: 0.1327\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.2666 - accuracy: 0.1561 - val_loss: 3.2853 - val_accuracy: 0.1460\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 95s 1s/step - loss: 3.2220 - accuracy: 0.1713 - val_loss: 3.2460 - val_accuracy: 0.1637\n",
      "InceptionV3          trained in 967.71 sec\n",
      "processing: DenseNet121\n",
      "fitting the model: DenseNet121\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 183s 3s/step - loss: 3.7247 - accuracy: 0.0437 - val_loss: 3.6158 - val_accuracy: 0.0221\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 158s 2s/step - loss: 3.5828 - accuracy: 0.0457 - val_loss: 3.5553 - val_accuracy: 0.0442\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 158s 2s/step - loss: 3.5308 - accuracy: 0.0569 - val_loss: 3.5159 - val_accuracy: 0.0619\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 157s 2s/step - loss: 3.4925 - accuracy: 0.0785 - val_loss: 3.4824 - val_accuracy: 0.0841\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 157s 2s/step - loss: 3.4602 - accuracy: 0.1109 - val_loss: 3.4525 - val_accuracy: 0.1239\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 156s 2s/step - loss: 3.4310 - accuracy: 0.1213 - val_loss: 3.4248 - val_accuracy: 0.1372\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 156s 2s/step - loss: 3.4025 - accuracy: 0.1384 - val_loss: 3.3978 - val_accuracy: 0.1416\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 158s 2s/step - loss: 3.3727 - accuracy: 0.1458 - val_loss: 3.3715 - val_accuracy: 0.1416\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 158s 2s/step - loss: 3.3423 - accuracy: 0.1556 - val_loss: 3.3440 - val_accuracy: 0.1549\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 155s 2s/step - loss: 3.3122 - accuracy: 0.1635 - val_loss: 3.3186 - val_accuracy: 0.1504\n",
      "DenseNet121          trained in 1597.9 sec\n",
      "processing: MobileNet\n",
      "fitting the model: MobileNet\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 47s 650ms/step - loss: 3.6865 - accuracy: 0.0378 - val_loss: 3.6398 - val_accuracy: 0.0354\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 40s 615ms/step - loss: 3.5901 - accuracy: 0.0501 - val_loss: 3.5772 - val_accuracy: 0.0442\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 40s 621ms/step - loss: 3.5202 - accuracy: 0.0663 - val_loss: 3.5218 - val_accuracy: 0.0487\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 40s 619ms/step - loss: 3.4617 - accuracy: 0.0810 - val_loss: 3.4778 - val_accuracy: 0.0708\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 40s 617ms/step - loss: 3.4097 - accuracy: 0.0938 - val_loss: 3.4303 - val_accuracy: 0.0796\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 40s 623ms/step - loss: 3.3589 - accuracy: 0.1080 - val_loss: 3.3832 - val_accuracy: 0.0841\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 40s 614ms/step - loss: 3.3073 - accuracy: 0.1291 - val_loss: 3.3361 - val_accuracy: 0.0929\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 39s 612ms/step - loss: 3.2550 - accuracy: 0.1463 - val_loss: 3.2885 - val_accuracy: 0.1195\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 40s 611ms/step - loss: 3.2022 - accuracy: 0.1664 - val_loss: 3.2396 - val_accuracy: 0.1504\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 40s 612ms/step - loss: 3.1493 - accuracy: 0.1934 - val_loss: 3.1904 - val_accuracy: 0.1637\n",
      "MobileNet            trained in 405.43 sec\n",
      "processing: VGG16\n",
      "fitting the model: VGG16\n",
      "Epoch 1/10\n",
      "52/64 [=======================>......] - ETA: 1:18 - loss: 3.6288 - accuracy: 0.0260"
     ]
    }
   ],
   "source": [
    "saved = buildTransferModels(TRAIN_IMAGES, VAL_IMAGES)\n",
    "print( \"A total of :\", saved,\" models from this build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e222a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(learning_rate=0.001)\n",
    "def get_model(model):\n",
    "# Load the pretained model\n",
    "    kwargs =    {'input_shape':(224, 224, 3),\n",
    "                'include_top':False,\n",
    "                'weights':'imagenet',\n",
    "                'pooling':'avg'}\n",
    "    \n",
    "    pretrained_model = model(**kwargs)\n",
    "    pretrained_model.trainable = False\n",
    "    \n",
    "    inputs = pretrained_model.input\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(36, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=sgd,#'adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6a8b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the models\n",
    "models = {\n",
    "    \"InceptionResNetV2\": {\"model\":tf.keras.applications.InceptionResNetV2, \"perf\":0},\n",
    "    \"ResNet101\": {\"model\":tf.keras.applications.ResNet101, \"perf\":0},\n",
    "    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n",
    "    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n",
    "    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n",
    "    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n",
    "}\n",
    "\n",
    "def buildTransferModels(TRAIN_IMAGES, VAL_IMAGES):\n",
    "\n",
    "  # Fit the models\n",
    "  takeNext = False\n",
    "  savedModels = 0\n",
    "  for name, model in models.items():\n",
    "    print(\"processing:\", name)\n",
    "    # Get the model\n",
    "    m = get_model(model['model'])\n",
    "    models[name]['model'] = m\n",
    "    start = perf_counter()\n",
    "    print('fitting the model:', name)\n",
    "    # Fit the model\n",
    "    try:\n",
    "        history = m.fit(TRAIN_IMAGES,validation_data=VAL_IMAGES,epochs=10,verbose=1) \n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "        print(\"Failed with:\",name,\" Transfer learnning\")\n",
    "        takeNext = True\n",
    "    if(takeNext is False):\n",
    "        # Sav the duration, the train_accuracy and the val_accuracy\n",
    "        duration = perf_counter() - start\n",
    "        duration = round(duration,2)\n",
    "        models[name]['perf'] = duration\n",
    "        print(f\"{name:20} trained in {duration} sec\")\n",
    "    \n",
    "        val_acc = history.history['val_accuracy']\n",
    "        models[name]['val_acc'] = [round(v,4) for v in val_acc]\n",
    "    \n",
    "        train_acc = history.history['accuracy']\n",
    "        models[name]['train_accuracy'] = [round(v,4) for v in train_acc]\n",
    "        m.save(name+\".h5\")\n",
    "        savedModels +=1\n",
    "  return savedModels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a629887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2515 2515\n",
      "./imagedata/hand2_q_right_seg_3_cropped.png q\n",
      "(2515, 2)\n",
      "(2515, 2) (2263, 2) (252, 2)\n",
      "Found 2037 validated image filenames belonging to 36 classes.\n",
      "Found 226 validated image filenames belonging to 36 classes.\n",
      "Found 252 validated image filenames belonging to 36 classes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get image file names and its classifiers\n",
    "image_dir = Path('./imagedata')\n",
    "labels = []\n",
    "# Get filepaths and labels\n",
    "filepaths =  os.listdir(image_dir)\n",
    "#filepaths = list(image_dir.glob(r'**/*.png'))\n",
    "count = 0 \n",
    "\n",
    "for filename in filepaths:\n",
    "    lbl = str(filename)[6:7]\n",
    "    labels.append(lbl)\n",
    "    filepaths[count] = './imagedata'+'/'+filename\n",
    "    count+=1\n",
    "\n",
    "print(len(filepaths), len(labels))\n",
    "print(filepaths[400], labels[400])\n",
    "\n",
    "\n",
    "# total 2515 images\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "# Concatenate filepaths and labels\n",
    "IMAGE_DF = pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "# Shuffle the DataFrame and reset index\n",
    "IMAGE_DF = IMAGE_DF.sample(frac=1).reset_index(drop = True)\n",
    "\n",
    "#le_Label_ACTIVE = LabelEncoder()\n",
    "#IMAGE_DF['Label'] = le_Label_ACTIVE.fit_transform(IMAGE_DF['Label'])\n",
    "\n",
    "#IMAGE_DF['Label'] = pd.Categorical(IMAGE_DF.Label)\n",
    "print(IMAGE_DF.shape)\n",
    "# Show the result\n",
    "IMAGE_DF.head(5)\n",
    "# Separate in train and test data\n",
    "TRAIN_DF, TEST_DF = train_test_split(IMAGE_DF, train_size=0.9, shuffle=True, random_state=1)\n",
    "print(IMAGE_DF.shape, TRAIN_DF.shape, TEST_DF.shape)\n",
    "# Create the generators\n",
    "train_generator,test_generator,TRAIN_IMAGES,VAL_IMAGES,TEST_IMAGES=create_gen(TRAIN_DF, TEST_DF)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ca0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the models\n",
    "models = {\n",
    "    \"InceptionResNetV2\": {\"model\":tf.keras.applications.InceptionResNetV2, \"perf\":0},\n",
    "    \"ResNet101\": {\"model\":tf.keras.applications.ResNet101, \"perf\":0},\n",
    "    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n",
    "    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n",
    "    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n",
    "    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n",
    "}\n",
    "\n",
    "def buildTransferModels(TRAIN_IMAGES, VAL_IMAGES, name):\n",
    "\n",
    "    model = models.get(name)\n",
    "    print(\"processing:\", name)\n",
    "    # Get the model\n",
    "    m = get_model(model['model'])\n",
    "    models[name]['model'] = m\n",
    "    start = perf_counter()\n",
    "    print('fitting the model:', name)\n",
    "    history = m.fit(TRAIN_IMAGES,\n",
    "                    validation_data=VAL_IMAGES,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                   batch_size = 32)\n",
    "\n",
    "    # Sav the duration, the train_accuracy and the val_accuracy\n",
    "    duration = perf_counter() - start\n",
    "    duration = round(duration,2)\n",
    "    models[name]['perf'] = duration\n",
    "    print(f\"{name:20} trained in {duration} sec\")\n",
    "    \n",
    "    val_acc = history.history['val_accuracy']\n",
    "    models[name]['val_acc'] = [round(v,4) for v in val_acc]\n",
    "    \n",
    "    train_acc = history.history['accuracy']\n",
    "    models[name]['train_accuracy'] = [round(v,4) for v in train_acc]\n",
    "    m.save(name+\"36.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81c5b2ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: InceptionResNetV2\n",
      "fitting the model: InceptionResNetV2\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 261s 4s/step - loss: 3.6702 - accuracy: 0.0515 - val_loss: 3.5829 - val_accuracy: 0.0796\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 220s 3s/step - loss: 3.5472 - accuracy: 0.0736 - val_loss: 3.5001 - val_accuracy: 0.0664\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 221s 3s/step - loss: 3.4818 - accuracy: 0.0781 - val_loss: 3.4555 - val_accuracy: 0.1018\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 220s 3s/step - loss: 3.4407 - accuracy: 0.0967 - val_loss: 3.4246 - val_accuracy: 0.1062\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 220s 3s/step - loss: 3.4056 - accuracy: 0.1159 - val_loss: 3.3924 - val_accuracy: 0.1239\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 220s 3s/step - loss: 3.3749 - accuracy: 0.1232 - val_loss: 3.3651 - val_accuracy: 0.1195\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 219s 3s/step - loss: 3.3460 - accuracy: 0.1379 - val_loss: 3.3363 - val_accuracy: 0.1327\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 219s 3s/step - loss: 3.3166 - accuracy: 0.1463 - val_loss: 3.3073 - val_accuracy: 0.1504\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 219s 3s/step - loss: 3.2874 - accuracy: 0.1527 - val_loss: 3.2779 - val_accuracy: 0.1549\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 219s 3s/step - loss: 3.2583 - accuracy: 0.1551 - val_loss: 3.2473 - val_accuracy: 0.1549\n",
      "InceptionResNetV2    trained in 2238.23 sec\n"
     ]
    }
   ],
   "source": [
    "buildTransferModels(TRAIN_IMAGES,VAL_IMAGES, \"InceptionResNetV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08bc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: ResNet101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 20:41:04.739573: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-08 20:41:04.739666: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the model: ResNet101\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 309s 5s/step - loss: 3.6332 - accuracy: 0.0329 - val_loss: 3.6008 - val_accuracy: 0.0265\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 287s 4s/step - loss: 3.5709 - accuracy: 0.0432 - val_loss: 3.5778 - val_accuracy: 0.0531\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 284s 4s/step - loss: 3.5550 - accuracy: 0.0560 - val_loss: 3.5690 - val_accuracy: 0.0398\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 299s 5s/step - loss: 3.5452 - accuracy: 0.0599 - val_loss: 3.5536 - val_accuracy: 0.0619\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 282s 4s/step - loss: 3.5344 - accuracy: 0.0717 - val_loss: 3.5485 - val_accuracy: 0.0531\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 280s 4s/step - loss: 3.5258 - accuracy: 0.0707 - val_loss: 3.5365 - val_accuracy: 0.0619\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 281s 4s/step - loss: 3.5175 - accuracy: 0.0707 - val_loss: 3.5312 - val_accuracy: 0.0752\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 282s 4s/step - loss: 3.5090 - accuracy: 0.0795 - val_loss: 3.5228 - val_accuracy: 0.0708\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 282s 4s/step - loss: 3.5013 - accuracy: 0.0761 - val_loss: 3.5210 - val_accuracy: 0.0354\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 283s 4s/step - loss: 3.4930 - accuracy: 0.0844 - val_loss: 3.5162 - val_accuracy: 0.0442\n",
      "ResNet101            trained in 2869.82 sec\n"
     ]
    }
   ],
   "source": [
    "buildTransferModels(TRAIN_IMAGES,VAL_IMAGES, \"ResNet101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: InceptionV3\n",
      "fitting the model: InceptionV3\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 110s 2s/step - loss: 3.6607 - accuracy: 0.0368 - val_loss: 3.6218 - val_accuracy: 0.0177\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 93s 1s/step - loss: 3.5344 - accuracy: 0.0515 - val_loss: 3.5478 - val_accuracy: 0.0398\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 93s 1s/step - loss: 3.4589 - accuracy: 0.0697 - val_loss: 3.4916 - val_accuracy: 0.0664\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 93s 1s/step - loss: 3.3942 - accuracy: 0.0913 - val_loss: 3.4290 - val_accuracy: 0.1150\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 92s 1s/step - loss: 3.3314 - accuracy: 0.1217 - val_loss: 3.3771 - val_accuracy: 0.1239\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 92s 1s/step - loss: 3.2698 - accuracy: 0.1492 - val_loss: 3.3159 - val_accuracy: 0.1549\n",
      "Epoch 7/10\n",
      "55/64 [========================>.....] - ETA: 11s - loss: 3.2109 - accuracy: 0.1773"
     ]
    }
   ],
   "source": [
    "buildTransferModels(TRAIN_IMAGES,VAL_IMAGES, \"InceptionV3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80eaeabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: DenseNet121\n",
      "fitting the model: DenseNet121\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 178s 2s/step - loss: 3.6562 - accuracy: 0.0422 - val_loss: 3.5955 - val_accuracy: 0.0354\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 153s 2s/step - loss: 3.5466 - accuracy: 0.0457 - val_loss: 3.5342 - val_accuracy: 0.0398\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 153s 2s/step - loss: 3.4956 - accuracy: 0.0599 - val_loss: 3.4945 - val_accuracy: 0.0619\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 151s 2s/step - loss: 3.4568 - accuracy: 0.0879 - val_loss: 3.4615 - val_accuracy: 0.0708\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 151s 2s/step - loss: 3.4223 - accuracy: 0.1046 - val_loss: 3.4276 - val_accuracy: 0.0841\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 148s 2s/step - loss: 3.3889 - accuracy: 0.1183 - val_loss: 3.3980 - val_accuracy: 0.0973\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 146s 2s/step - loss: 3.3570 - accuracy: 0.1306 - val_loss: 3.3704 - val_accuracy: 0.0885\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 151s 2s/step - loss: 3.3251 - accuracy: 0.1409 - val_loss: 3.3436 - val_accuracy: 0.0973\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 151s 2s/step - loss: 3.2939 - accuracy: 0.1483 - val_loss: 3.3174 - val_accuracy: 0.1195\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 152s 2s/step - loss: 3.2626 - accuracy: 0.1625 - val_loss: 3.2902 - val_accuracy: 0.1283\n",
      "DenseNet121          trained in 1536.1 sec\n"
     ]
    }
   ],
   "source": [
    "buildTransferModels(TRAIN_IMAGES,VAL_IMAGES, \"DenseNet121\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f4ec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: MobileNet\n",
      "fitting the model: MobileNet\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 47s 649ms/step - loss: 3.6730 - accuracy: 0.0344 - val_loss: 3.5447 - val_accuracy: 0.0487\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 39s 606ms/step - loss: 3.5695 - accuracy: 0.0550 - val_loss: 3.4907 - val_accuracy: 0.0885\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 39s 604ms/step - loss: 3.5218 - accuracy: 0.0746 - val_loss: 3.4571 - val_accuracy: 0.1062\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 39s 600ms/step - loss: 3.4863 - accuracy: 0.0908 - val_loss: 3.4268 - val_accuracy: 0.1062\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 39s 599ms/step - loss: 3.4542 - accuracy: 0.1105 - val_loss: 3.3978 - val_accuracy: 0.1504\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 38s 594ms/step - loss: 3.4222 - accuracy: 0.1325 - val_loss: 3.3678 - val_accuracy: 0.1460\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 39s 608ms/step - loss: 3.3901 - accuracy: 0.1487 - val_loss: 3.3358 - val_accuracy: 0.2035\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 39s 604ms/step - loss: 3.3572 - accuracy: 0.1649 - val_loss: 3.3039 - val_accuracy: 0.2124\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 39s 613ms/step - loss: 3.3238 - accuracy: 0.1802 - val_loss: 3.2727 - val_accuracy: 0.2124\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 39s 604ms/step - loss: 3.2898 - accuracy: 0.1964 - val_loss: 3.2402 - val_accuracy: 0.2434\n",
      "MobileNet            trained in 398.17 sec\n"
     ]
    }
   ],
   "source": [
    "buildTransferModels(TRAIN_IMAGES,VAL_IMAGES, \"MobileNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28e16a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: VGG16\n",
      "fitting the model: VGG16\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 455s 7s/step - loss: 3.6359 - accuracy: 0.0270 - val_loss: 3.6567 - val_accuracy: 0.0221\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 451s 7s/step - loss: 3.6127 - accuracy: 0.0270 - val_loss: 3.6355 - val_accuracy: 0.0221\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 451s 7s/step - loss: 3.5988 - accuracy: 0.0285 - val_loss: 3.6201 - val_accuracy: 0.0221\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 452s 7s/step - loss: 3.5889 - accuracy: 0.0383 - val_loss: 3.6082 - val_accuracy: 0.0265\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 456s 7s/step - loss: 3.5816 - accuracy: 0.0432 - val_loss: 3.6005 - val_accuracy: 0.0398\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 454s 7s/step - loss: 3.5767 - accuracy: 0.0481 - val_loss: 3.5955 - val_accuracy: 0.0398\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 452s 7s/step - loss: 3.5730 - accuracy: 0.0501 - val_loss: 3.5912 - val_accuracy: 0.0442\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 452s 7s/step - loss: 3.5696 - accuracy: 0.0545 - val_loss: 3.5877 - val_accuracy: 0.0442\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 453s 7s/step - loss: 3.5666 - accuracy: 0.0515 - val_loss: 3.5852 - val_accuracy: 0.0442\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 453s 7s/step - loss: 3.5639 - accuracy: 0.0530 - val_loss: 3.5827 - val_accuracy: 0.0442\n",
      "VGG16                trained in 4528.86 sec\n"
     ]
    }
   ],
   "source": [
    "buildTransferModels(TRAIN_IMAGES,VAL_IMAGES, \"VGG16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ab4abfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 34s 3s/step\n"
     ]
    }
   ],
   "source": [
    "m =  models[\"InceptionResNetV2\"]['model']\n",
    "ypredictionRN2 = m.predict(TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f55bd52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 [0.01390894 0.03801033 0.03917402 0.04834156 0.02821347 0.03703311\n",
      " 0.04156258 0.0402364  0.02876344 0.01913362 0.02182228 0.02284929\n",
      " 0.02479278 0.0215046  0.03413737 0.04300565 0.02044649 0.0157331\n",
      " 0.03517412 0.03720588 0.02197585 0.02556166 0.03123729 0.01775581\n",
      " 0.02419085 0.01927164 0.01762393 0.02654485 0.02252811 0.02427919\n",
      " 0.02323967 0.01771199 0.04088639 0.01941895 0.02193448 0.03479033]\n",
      "252 36\n",
      "[32, 5, 5, 27]\n"
     ]
    }
   ],
   "source": [
    "print(len(ypredictionRN2), ypredictionRN2[52])\n",
    "def getSinglePredictedMaxColumn (prediction):\n",
    "    num_rows, num_cols = prediction.shape\n",
    "    print(num_rows, num_cols)\n",
    "    new_list = []\n",
    "    for i in range(num_rows):\n",
    "        column_list = list(prediction[i])\n",
    "        max_index = column_list.index(max(column_list))\n",
    "        new_list.append(max_index)\n",
    "\n",
    "    return(new_list)\n",
    "newpred = getSinglePredictedMaxColumn(ypredictionRN2)\n",
    "print(newpred[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7761f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
